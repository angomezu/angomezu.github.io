<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Angel A. Barrera Gomez</title>
  <link rel="stylesheet" href="assets/style.css" />
</head>
<body>
  <div class="wrap">
    <aside class="side">
      <img class="photo" src="assets/prof_pic.png" alt="Angel portrait" />
      <h1>Angel A. Barrera Gomez</h1>
      <p class="role">B.E. Computer Systems Engineering • M.S. Data Science • 3D Perception • Embodied AI</p>

      <div class="contact">
        <a href="mailto:angomezu@gmail.com">angomezu@gmail.com</a>
        <div class="muted">Johnson City, TN, USA</div>
      </div>

      <nav class="nav">
        <a href="assets/Angel_Barrera_Resume.pdf">
          <span style="display:inline-flex; align-items:center; gap:6px;">
            <!-- Document icon -->
            <svg width="16" height="16" viewBox="0 0 24 24" fill="none"
                 xmlns="http://www.w3.org/2000/svg">
              <path d="M6 2h9l5 5v15a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2z"
                    stroke="currentColor" stroke-width="2"/>
              <path d="M14 2v6h6" stroke="currentColor" stroke-width="2"/>
            </svg>
            CV
          </span>
        </a>
        <a href="https://github.com/angomezu" target="_blank">
          <svg width="16" height="16" viewBox="0 0 24 24" style="vertical-align:middle;margin-right:6px;">
            <path fill="currentColor"
              d="M12 .5C5.7.5.5 5.7.5 12c0 5.1 3.3 9.4 7.9 10.9.6.1.8-.3.8-.6v-2.1c-3.2.7-3.9-1.4-3.9-1.4-.5-1.2-1.2-1.6-1.2-1.6-1-.7.1-.7.1-.7 1.1.1 1.7 1.1 1.7 1.1 1 .1.8 2.2 3.3 1.6.1-.8.4-1.3.7-1.6-2.6-.3-5.3-1.3-5.3-5.8 0-1.3.5-2.3 1.2-3.1-.1-.3-.5-1.5.1-3.1 0 0 1-.3 3.2 1.2a11 11 0 0 1 5.8 0c2.2-1.5 3.2-1.2 3.2-1.2.6 1.6.2 2.8.1 3.1.8.8 1.2 1.8 1.2 3.1 0 4.5-2.7 5.5-5.3 5.8.4.3.8 1 .8 2.1v3.1c0 .3.2.7.8.6A11.5 11.5 0 0 0 23.5 12C23.5 5.7 18.3.5 12 .5z"/>
          </svg>
          GitHub
        </a>
        <a href="https://www.linkedin.com/in/angomezu/" target="_blank">
          <svg width="16" height="16" viewBox="0 0 24 24" style="vertical-align:middle;margin-right:6px;">
            <path fill="currentColor"
              d="M4.98 3.5a2.5 2.5 0 1 0 0 5a2.5 2.5 0 0 0 0-5zM3 8.98h3.96V21H3zM9.5 8.98H13v1.64h.05c.49-.93 1.69-1.91 3.48-1.91C20.1 8.71 21 11 21 14.1V21h-3.96v-6.1c0-1.45-.03-3.31-2.02-3.31-2.02 0-2.33 1.58-2.33 3.21V21H9.5z"/>
          </svg>
          LinkedIn
        </a>
      </nav>
    </aside>

    <main class="main">
      <section id="about">
        <h2>About</h2>
        <p>
          I’m a data scientist and engineer with a research direction in perception and learning for embodied intelligence.
          My core technical foundation is 3D perception and geometric deep learning, where I work with real sensor data
          (LiDAR point clouds, structured logs) to build systems that are robust, testable, and useful in practice.
        </p>
        <p>
          I’m especially interested in how robots can learn reliable representations under partial observability (occlusion),
          noise, and domain shift, and how those representations can later support data-efficient learning for manipulation
          and human–robot collaboration. I have not yet worked directly on embodied AI policy learning, but my current work is intentionally building the
          perception + evaluation foundation needed to transition there.
        </p>
      </section>

      <section id="research">
        <h2>Research</h2>

        <p class="muted">
          PhD identity: Researcher in perception and learning for embodied intelligence
        </p>

        <p>
          Building on my experience with 3D point-cloud-based deep learning on real sensor data, I’m interested in
          perception-driven robotic manipulation. In particular, I want to study how geometric 3D representations can be learned robustly
          under partial observability and noise, and how these representations can support data-efficient learning and
        human–robot interaction in real-world settings.
        </p>

        <ul>
          <li><strong>Perception:</strong> learning action-relevant representations from point clouds; robustness under occlusion, noise, and domain shift; sensor fusion (RGB + depth + LiDAR).</li>
          <li><strong>Learning in the real world:</strong> self-supervised perception; sim-to-real transfer; learning with partial/weak labels.</li>
          <li><strong>Human–robot interaction:</strong> vision-based intent recognition; safe shared workspaces; multimodal perception (gesture + motion + scene).</li>
          <li><strong>Embodied multimodal intelligence:</strong> grounding vision/language for robotics; policies conditioned on visual + semantic inputs.</li>
        </ul>

        <p>
          <strong>Bridge experience:</strong> My ORNL work connects directly to these goals: geometry-aware perception from LiDAR point clouds,
          handling real-world noise and imbalance, and building rigorous evaluation/uncertainty pipelines that matter for downstream action.
        </p>
      </section>

      <section id="projects">
        <h2>Selected Projects</h2>

        <h3>3D LiDAR Plant Organ Segmentation</h3>
        <div class="img-grid">
          <a href="assets/prediction.gif" target="_blank">
            <img src="assets/prediction.gif" alt="Segmentation result">
          </a>
        
          <a href="assets/LiDAR_Scan.jpg" target="_blank">
            <img src="assets/LiDAR_Scan.jpg" alt="3D LiDAR scan">
          </a>
        
          <a href="assets/APPL_Facility.png" target="_blank">
            <img src="assets/APPL_Facility.png" alt="Scanning facility">
          </a>
        </div>
          
          <p class="muted">Point cloud segmentation • geometric deep learning • robust evaluation</p>
          <ul>
            <li>Developed a geometry-aware deep learning pipeline for plant organ segmentation from 3D LiDAR point clouds.</li>
            <li>Engineered local geometric descriptors and evaluated under class imbalance and partial observability (occlusion).</li>
            <li>Current best configuration reports ~65.6% mIoU and ~82% stem recall.</li>
          </ul>
          <p><a href="https://github.com/angomezu/geometric-deep-learning-plant-organ-segmentation.git">Project page →</a></p>
        </div>

        <div class="card">
          <h3>EEG Motor Imagery Classification (Conv1D CNN)</h3>
          <p class="muted">Signal processing • deep learning • generalization challenges</p>
          <ul>
            <li>Built a full pipeline to classify left vs. right fist motor imagery from PhysioNet EEG (20 subjects, 64 channels, 160 Hz).</li>
            <li>Compared CSP-based baselines vs deep models; final 2-layer Conv1D CNN reached <strong>76.61% accuracy</strong> (ROC-AUC 0.79).</li>
            <li>Analyzed subject-wise generalization (avg. accuracy 51.8%) and documented future work for domain adaptation and transfer learning.</li>
          </ul>
          <p><a href="https://github.com/angomezu/bci-motor-classification-cnn">Repository →</a></p>
        </div>

        <div class="card">
          <h3>CRM → PostgreSQL ETL for Real Estate Analytics (Render + Power BI)</h3>
          <p class="muted">Data engineering • event-driven ingestion • BI system design</p>
          <ul>
            <li>Built a cloud BI system that streams CRM events via Flask webhooks into PostgreSQL on Render.</li>
            <li>Combined real-time ingestion with scheduled incremental API sync; maintained full event history for auditability.</li>
            <li>Powered 8 interactive Power BI dashboards with a standardized DAX measure library (60+ measures) and optimized schema/indexing.</li>
          </ul>
          <p><a href="https://github.com/angomezu/CRM-to-PostgreSQL-ETL-for-Real-Estate-Analytics">Repository →</a></p>
        </div>

        <div class="card">
          <h3>Predicting Hospital Readmission (Logistic Regression in R)</h3>
          <p class="muted">Statistical modeling • interpretability • healthcare analytics</p>
          <ul>
            <li>Modeled hospital readmission risk on 25,000 patient records using logistic regression with stepwise AIC selection.</li>
            <li>Achieved ~61.7% test accuracy (AUC 0.662) and identified key drivers (length of stay, prior visits, medication/labs, age).</li>
          </ul>
          <p><a href="https://github.com/angomezu/Predicting-Hospital-Readmission">Repository →</a></p>
        </div>

        <div class="card">
          <h3>Apex Capital: Financial + ESG Decision Support System (Power BI + Python + MySQL)</h3>
          <p class="muted">ETL + data integrity • decision support • analytics engineering</p>
          <ul>
            <li>Built a full-stack prototype DSS for “dual-mandate” investing (value + ESG), merging multi-source financial and ESG datasets.</li>
            <li>Implemented an ETL workflow with data quality discovery (e.g., mismatched tickers, missing ESG coverage) and schema-enforced loading into MySQL.</li>
            <li>Delivered a Power BI “financial terminal” dashboard with context-aware DAX measures (sector benchmarking, investable gating, ESG null handling).</li>
          </ul>
          <p><a href="https://github.com/angomezu/Financial-Analytics-DSS">Repository →</a></p>
        </div>

        <div class="card">
          <h3>PDF Charge Parser (Python GUI)</h3>
          <p class="muted">Automation tool • PDF parsing • Excel reporting</p>
          <ul>
            <li>Built a lightweight desktop tool to parse telecom billing PDFs and extract roaming/long-distance charges into a clean Excel report.</li>
            <li>Implemented robust parsing with PyMuPDF and rule-based extraction, wrapped in a simple Tkinter GUI with progress tracking.</li>
          </ul>
          <p><a href="https://github.com/angomezu/pdf_charge_parser">Repository →</a></p>
        </div>
      </section>

      <section id="goals">
        <h2>Goals</h2>
        <ul>
          <li><strong>Research direction:</strong> grow from geometry-aware 3D perception into embodied AI (perception → learning → action), with an emphasis on robustness and data efficiency.</li>
          <li><strong>Near-term:</strong> turn the ORNL 3D LiDAR project into a formal research write-up and submission.</li>
          <li><strong>Skill ramp:</strong> deepen PyTorch and modern CV tooling toward robotics-relevant perception (3D, multimodal fusion, self-supervised learning).</li>
          <li><strong>Career:</strong> pursue roles and/or a PhD path aligned with perception for robotics and real-world intelligent systems.</li>
        </ul>
      </section>

      <section id="updates">
        <h2>Updates</h2>
        <ul class="updates">
          <li><span class="date">2025-12</span> Personal site launched and project portfolio consolidated.</li>
          <li><span class="date">2026-01</span> Preparing ORNL 3D LiDAR project write-up for submission.</li>
          <li><span class="date">2026</span> Transitioning focus toward embodied AI: robust 3D perception → learning for manipulation/HRI.</li>
        </ul>
      </section>

      <footer class="footer">
        © <span id="y"></span> Angel A. Barrera Gomez
      </footer>
    </main>
  </div>

  <script>document.getElementById("y").textContent = new Date().getFullYear();</script>
</body>
</html>
